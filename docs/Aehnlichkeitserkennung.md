# Ã„hnlichkeitserkennung bei Artikel-Deduplizierung

## Ãœbersicht

Die Ã„hnlichkeitserkennung wird verwendet, um doppelte oder sehr Ã¤hnliche Artikel beim Import automatisch zu erkennen und zu entfernen. Dies verhindert, dass verschiedene Versionen desselben Artikels mehrfach in der Datenbank landen.

---

## Technische Implementierung

### SequenceMatcher-Algorithmus

Die Erkennung nutzt Pythons `difflib.SequenceMatcher`, der auf dem **Longest Common Subsequence (LCS)** Algorithmus basiert:

```python
from difflib import SequenceMatcher

def similarity(a, b):
    """Berechnet Ã„hnlichkeit zwischen zwei Strings (0.0 - 1.0)"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()
```

### Funktionsweise

**1. Longest Common Subsequence (LCS)**
- Findet die lÃ¤ngste gemeinsame Teilsequenz zwischen zwei Strings
- Beispiel:
  ```
  Text A: "Lindner vom Finanzminister zum Mediator"
  Text B: "Lindner vom Ex-Finanzminister zum Mediator"
  
  Gemeinsam: "Lindner vom ", "Finanzminister zum Mediator"
  ```

**2. Ratio-Berechnung**
```
ratio = 2.0 Ã— M / T

Wobei:
  M = LÃ¤nge der gemeinsamen Zeichen
  T = GesamtlÃ¤nge beider Strings
```

**3. Ergebnis**
- `0.0` = VÃ¶llig unterschiedlich
- `1.0` = Identisch
- `0.95` = 95% Ã¤hnlich (sehr wahrscheinlich Duplikat)

---

## Praktische Beispiele

### Identische Artikel (100%)

```python
titel1 = "Fake Daily â€“ Die Wahrheit, die keiner hÃ¶ren will!"
titel2 = "Fake Daily â€“ Die Wahrheit, die keiner hÃ¶ren will!"

similarity(titel1, titel2)
# â†’ 1.00 (100% identisch)
```

### Minimale Unterschiede (98%)

```python
titel1 = "ğŸ“° Fake Daily â€“ Die Wahrheit, die keiner hÃ¶ren will!"
titel2 = "Fake Daily â€“ Die Wahrheit, die keiner hÃ¶ren will!"

similarity(titel1, titel2)
# â†’ 0.98 (nur Emoji fehlt)
```

### EingefÃ¼gter Text (96%)

```python
titel1 = "Deutschland unregierbar â€“ Kaiser von Ã–sterreich bietet Eingliederung"
titel2 = "Deutschland unregierbar â€“ Kaiser â€Chef" von Ã–sterreich bietet Eingliederung"

similarity(titel1, titel2)
# â†’ 0.96 (nur â€Chef" eingefÃ¼gt)
```

### Leicht unterschiedlich (88%)

```python
titel1 = "Lindner vor neuer Karriere: Vom Finanzminister zum Mediator"
titel2 = "Lindner vor neuer Karriere: Vom Ex-Finanzminister zum Mediator"

similarity(titel1, titel2)
# â†’ 0.94 (nur "Ex-" eingefÃ¼gt)
```

---

## Schwellwerte beim Deduplizieren

### Titel-Vergleich

```python
if similarity(title1, title2) > 0.95:
    # â†’ Als Duplikat markieren
    # â†’ LÃ¤ngere Version behalten
```

**BegrÃ¼ndung:** Bei >95% Ã„hnlichkeit handelt es sich fast immer um denselben Artikel mit minimalen Ã„nderungen (Tippfehler, Formatierung, etc.)

### Content-Vergleich

```python
content1_preview = content1[:500]  # Erste 500 Zeichen
content2_preview = content2[:500]

if similarity(content1_preview, content2_preview) > 0.90:
    # â†’ Als Duplikat markieren
```

**BegrÃ¼ndung:** Content-Vergleich ist weniger streng (90% statt 95%), da:
- Artikel oft mit gleicher Einleitung beginnen
- Nur Preview verglichen wird (Performance)
- Geringe Unterschiede im Haupttext hÃ¤ufiger vorkommen

### Kombination

Ein Artikel gilt als Duplikat wenn **ENTWEDER**:
- Titel >95% Ã¤hnlich **ODER**
- Content >90% Ã¤hnlich

Bei Duplikaten wird die **lÃ¤ngere Version** behalten.

---

## Performance-Ãœberlegungen

### KomplexitÃ¤t

- **Zeit:** O(n Ã— m) fÃ¼r jeden Vergleich
  - n = LÃ¤nge String A
  - m = LÃ¤nge String B
  
- **Gesamt:** O(NÂ²) fÃ¼r N Artikel
  - Bei 264 Artikeln: ~35.000 Vergleiche
  - Bei 1000 Artikeln: ~500.000 Vergleiche

### Optimierungen

**1. Preview fÃ¼r Content**
```python
# Statt ganzen Artikel (5000+ Zeichen):
content_preview = content[:500]
# â†’ 10x schneller
```

**2. Nur generische Tags prÃ¼fen**
```python
generic_tags = {'satire', 'fake-daily', 'Satire'}
if set(current_tags).issubset(generic_tags):
    # Nur diese Artikel prÃ¼fen
```

**3. Early Exit**
```python
for existing in unique_articles:
    if similarity(title1, title2) > 0.95:
        break  # Erstes Duplikat gefunden, fertig
```

---

## Vorteile & Nachteile

### âœ… Vorteile

- **Erkennt Tippfehler:** "Finanzminister" vs "Finanzminiter"
- **Ignoriert GroÃŸ/Klein:** Durch `.lower()`
- **Funktioniert bei Umstellungen:** Teilweise
- **Schnell genug:** O(nÃ—m) ist akzeptabel fÃ¼r kleine DatensÃ¤tze
- **Keine Konfiguration:** Funktioniert out-of-the-box

### âŒ Nachteile

- **Keine Synonyme:** "Auto" vs "Fahrzeug" = unterschiedlich
- **Reihenfolge wichtig:** "A B C" vs "C B A" = geringe Ã„hnlichkeit
- **Falsch-Positive bei kurzen Texten:** "Die Wahrheit" Ã¤hnlich zu "Die LÃ¼ge"
- **Keine semantische Analyse:** Versteht keinen Kontext

---

## Alternative AnsÃ¤tze

FÃ¼r grÃ¶ÃŸere Projekte oder hÃ¶here Anforderungen:

### 1. Fuzzy Matching (fuzzywuzzy)
```python
from fuzzywuzzy import fuzz

ratio = fuzz.ratio(text1, text2)
token_sort_ratio = fuzz.token_sort_ratio(text1, text2)
```
**Vorteile:** Robuster bei Umstellungen

### 2. Levenshtein-Distanz
```python
from Levenshtein import distance

dist = distance(text1, text2)
```
**Vorteile:** Exakte Anzahl an Ã„nderungen

### 3. Embeddings / Semantische Ã„hnlichkeit
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
embeddings = model.encode([text1, text2])
similarity = cosine_similarity(embeddings[0], embeddings[1])
```
**Vorteile:** Versteht Bedeutung, erkennt Paraphrasen

---

## Implementierung im Projekt

### Script: `clean_articles.py`

```python
# 1. Artikel laden
articles = load_articles()

# 2. Duplikate finden
unique_articles = []
for article in articles:
    is_duplicate = False
    
    for existing in unique_articles:
        # Titel-Check
        if similarity(article['title'], existing['title']) > 0.95:
            is_duplicate = True
            break
        
        # Content-Check
        if similarity(article['content'][:500], existing['content'][:500]) > 0.90:
            is_duplicate = True
            break
    
    if not is_duplicate:
        unique_articles.append(article)
```

### Ergebnis

```
Original: 358 Artikel
Nach Filterung: 273 Artikel (85 Nicht-Artikel entfernt)
Nach Deduplizierung: 264 Artikel (9 Duplikate entfernt)

Gesamt entfernt: 94 EintrÃ¤ge
```

---

## Integration in das CMS

Die Ã„hnlichkeitserkennung ist direkt in die Import-API integriert und wird bei jedem Import automatisch ausgefÃ¼hrt.

### API-Integration

**Endpoint:** `POST /cms/admin/api/import/articles`

**Import-Workflow:**
1. âœ… **Exakter Titel-Match**: PrÃ¼fen ob Artikel mit identischem Titel existiert
2. âœ… **Similarity Detection**: Wenn kein exakter Match â†’ alle Artikel nach Ã„hnlichkeit durchsuchen
3. âœ… **Duplicate Handling**: Bei >95% Titel-Ã„hnlichkeit oder >90% Content-Ã„hnlichkeit â†’ als Duplikat behandeln
4. âœ… **Timestamp-Vergleich**: Bei Duplikaten â†’ neuere Version behalten (via `updated_at`)

### Code-Beispiel (app.py)

```python
from difflib import SequenceMatcher

def similarity(a: str, b: str) -> float:
    """Calculate similarity ratio using SequenceMatcher"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def are_similar_articles(new_article: dict, existing_article: dict,
                         title_threshold: float = 0.95,
                         content_threshold: float = 0.90) -> bool:
    """Check if two articles are similar"""
    title_sim = similarity(new_article.get('title', ''), 
                          existing_article.get('title', ''))
    
    if title_sim > title_threshold:
        return True
    
    # Check content similarity (first 500 chars)
    content1 = new_article.get('content', '')[:500]
    content2 = existing_article.get('content', '')[:500]
    content_sim = similarity(content1, content2)
    
    if content_sim > content_threshold:
        return True
    
    return False

@app.route('/admin/api/import/articles', methods=['POST'])
def import_articles_json():
    """Import articles with similarity detection"""
    # ... (siehe web/app.py Zeile 744+)
    
    # PrÃ¼fen ob Artikel mit diesem Titel bereits existiert
    existing = db.get_article_by_title(title)
    
    # Wenn kein exakter Match: Ã„hnlichkeitserkennung
    if not existing:
        all_articles = db.get_all_articles()
        for article in all_articles:
            if are_similar_articles(article_data, article):
                existing = article
                break
    
    # Bei Duplikat: Timestamp-Vergleich
    if existing:
        # ... Update-Logik ...
```

### Tests

VollstÃ¤ndige Unit-Tests in `tests/test_similarity_detection.py`:

```bash
pytest tests/test_similarity_detection.py -v
```

**Test-Kategorien:**
- âœ… Similarity-Funktion (identisch, case-insensitive, unterschiedlich)
- âœ… Threshold-Beispiele aus Dokumentation
- âœ… Article-Vergleich (exakt, Ã¤hnlich, unterschiedlich)
- âœ… Custom Thresholds
- âœ… Performance-Tests
- âœ… Edge Cases
- âœ… Import-Szenarien (Duplikat, Emoji, Update, Ã¤hnlich aber anders)

## Vorteile der Integration

**âœ… Automatische Erkennung** - Keine manuelle PrÃ¼fung mehr nÃ¶tig

**âœ… Flexibel** - Thresholds kÃ¶nnen angepasst werden (derzeit 95%/90%)

**âœ… Performance** - Vergleich in <10ms pro Artikel-Paar

**âœ… Logging** - Ã„hnlichkeiten werden geloggt fÃ¼r Review

**âœ… Dokumentiert** - Thresholds und Algorithmus sind nachvollziehbar

## Verwendung

### Import via API

```bash
curl -X POST http://stage:5001/cms/admin/api/import/articles \
  -H "Content-Type: application/json" \
  -d @articles.json
```

**Response:**
```json
{
  "success": true,
  "imported": 198,
  "skipped": 66,  # EnthÃ¤lt auch via Similarity erkannte Duplikate
  "updated": 0,
  "errors": []
}
```

### Import via Python-Skript

Automatisiertes Workflow-Skript in `import/import_workflow.py`:

```bash
cd import
python3 import_workflow.py \
  --input conversations.json \
  --output articles-import.json \
  --title-threshold 0.95 \
  --content-threshold 0.90
```

**Features:**
- LÃ¤dt Conversations aus ChatGPT Export
- Extrahiert Artikel
- Dedupliziert via Similarity Detection
- Konvertiert zu CMS-Format
- Importiert direkt zur API

## Wartung und Anpassung

### Thresholds anpassen

Wenn zu viele False Positives (fÃ¤lschlich als Duplikat erkannt):
- Title-Threshold erhÃ¶hen: `0.95` â†’ `0.97`
- Content-Threshold erhÃ¶hen: `0.90` â†’ `0.93`

Wenn Duplikate nicht erkannt werden:
- Title-Threshold senken: `0.95` â†’ `0.92`
- Content-Threshold senken: `0.90` â†’ `0.85`

### Logging Ã¼berprÃ¼fen

Bei Similarity-Matches wird geloggt:
```
Similarity detected: 'Neuer Titel' ~ 'Existierender Titel' (using existing)
```

Logs finden in `logs/app.log`

## Best Practices

### âœ… Empfohlene Schwellwerte

| Vergleich | Schwellwert | Verwendung |
|-----------|-------------|------------|
| Titel | >95% | Duplikat-Erkennung |
| Titel | >80% | Warnung/PrÃ¼fung |
| Content (Preview) | >90% | Duplikat-Erkennung |
| Content (Voll) | >85% | FÃ¼r kleine DatensÃ¤tze |

### âš ï¸ GrenzfÃ¤lle manuell prÃ¼fen

Bei Ã„hnlichkeit zwischen 85-95%:
- Manuell anschauen
- KÃ¶nnte legitim verschiedene Artikel sein
- Oder unterschiedliche Versionen

### ğŸ” Logging

```python
if 0.85 < similarity_ratio < 0.95:
    logger.warning(f"Potentielles Duplikat: {title1} vs {title2} ({similarity_ratio:.0%})")
```

---

## Siehe auch

- **ChatGPT Import:** `import/clean_articles.py`
- **Duplikate-Check:** `import/check_duplicates.py`
- **Auto-Tagging:** `docs/Auto-Tagging.md`
- **Python difflib:** https://docs.python.org/3/library/difflib.html
